1. Different file formats.
    text file using as-textfile (default)
    sequence file using as-sequencefile
    Avro file using as-avrofile
    parquet file using as-parquetfile
    
 2. Different field and line delimiters.
    --fields-terminated-by ,  (default)
    --fields-terminated-by "\t"  (tab delimiter)
    --lines-terminated-by "\n"  (default)
    
 3. Null columns handling
   -- null-string ""
   --null-non-string -1
   
 4. compression.
   --compress to enable the compression and then pass the following commpression algorithm
   --compression-codec to use specific compression algorithm
   --compression-codec org.apache.hadoop.io.compress.GzipCodec
   --compression-codec org.apache.hadoop.io.compress.SnappyCodec

 5. Adding WHERE clause in query sqoop import and we need to use split-by column
    --query "select * from orders where \$CONDITIONS and order_date like '2014-01%'"
    --split-by order_id

 6. Sample from Cloudera certification page. 
    import all of customer table's data from mysql problem1 database on gateway node to hdfs location /user/cert/problem1/soultion .
    username and password are cloudera. Output should be text and , delimited columns.
      sqoop import \
      --connect jdbc:mysql://gateway/problem1 \
      --username cloudera --password cloudera \
      --table customer \
      --target-dir /user/cert/problem1/solution
       No need to specify file format and fields terminated as asked are default parameters.
       
  7. For hive import, first data is loaded onto hdfs location /user/cloudera/table-name and then imports into hive database. if table is
     not created, then hive import creates the table. If the import is successful, then the data in the above hdfs staging location delete.
     Default fields seperator in hive is ascii value 1 or Ctrl + A character.
     
 8. sqoop import command to import orders table to hive with delimiter change (Ctrl + a) to , and a different hive table name.
    sqoop import \
    --connect jdbc:mysql://localhost/retail_db \
    --table orders \
    --username retail_dba --password cloudera \
    --hive-import \
    --hive-database sqoop_examples \
    --hive-table orders_ex \
    --fields-terminated-by ,

 9. import-all-tables - warehouse-dir is mandatory here.
     sqoop import-all-tables \
      --connect jdbc:mysql://localhost/retail_db \
      --username retail_dba --password cloudera \
      --warehouse-dir /user/cloudera/Practice/sqoop/data/retail_db
     
      
     
