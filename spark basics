1. Reading from local file system. First use Source.fromFile from scala API and then create RDD
    val employeeFile = scala.io.Source.fromFile("/home/cloudera/data/employee/part-m-00000").getLines.toList
    val employeeRDD = sc.parallelize(emplyeeFile)
    
    val employeeRDD = sc.textFile("file:///home/cloudera/data/employee")

2. Reading from HDFS.
 (a) Read a text file. No need to specify compression codec.
     val employeeRDD = sc.textFile("/user/cloudera/data/employee")
  (b) Reading a sequence file.
     val employeeRDD = sc.sequenceFile("/user/cloudera/data/employee")
  (c) Reading using a DataFrame. load takes two parameters (path and file formats), 
      where as read has submethod for file format and takes one parameter (path).
     
     val employeeRDD = sqlContext.load("/user/cloudera/data/employee", "text")
     or
     val employeeRDD = sqlContext.read.text("/user/cloudera/data/employee")
     *above text can be replaced with different file formats like json, etc..
     
     val employeeRDD = sqlContext.read.format("com.databricks.spark.avro").load("/user/cloudera/data/employee1/000000_0.avro")
     
 3. Reading from s3.
     val employeeRDD = sc.textFile("s3n://bucket/emplyoee.data")
     
 4. Storing to HDFC. Only text File format needs delimiters and other formats such as jSon, parquet do not need delimiters to specify.
   (a) save as text file
      RDD.saveAsTextFile("/user/cloudera/Practice/employee")
      
      with compression codec
      RDD.saveAsTextFile("/user/cloudera/Practice/employee",classOf[org.apache.hadoop.io.compress.SnappyCodec])
   
   (b) Saving other formats such as ORC, parquet, json, need to use the APIs on the Data Frames.
       Creating DF first
      val employeeDF = sqlContext.read.json("/user/cloudera/data_json/employee")
        or
      val employeeDF = sqlContext.load("/user/cloudera/data_json/employee", "json")
      
      //save as json
      employeeDF.save("/user/cloudera/Practice/employee_json", "json")
      //save as parquet
      employeeDF.save("/user/cloudera/Practice/employee_parquet", "parquet")
      //save as ORC
      employeeDF.save("/user/cloudera/Practice/employee_orc", "orc")
      //save as avro
      employeeDF.write.format("com.databricks.spark.avro").save("/user/cloudera/Practice/employee_avro")   
      //save using write method.
      employeeDF.write.orc("/user/cloudera/Practice/employee_orc")
     
4. Converting List into RDD.
    val l_RDD = sc.parallelize("list1")

5. Converting RDD into DataFrame and registering as Temporary table. Note registerTempTable is deprecated and need to use 
   createOrReplaceTempView in 2.0 version.
    val ordersRDD = sc.textFile("/user/cloudera/data/orders")
    val ordersDF = ordersRDD.map(rec => {
      val o = rec.split(",")
      o(0).toInt, o(1), o(2).toInt, o(3)}).toDF("order_id", "order_date", order_customer_id", "order_status")
      
      ordersDF.registerTempTable("orders")
      
 6. Converting DataFrame into RDD. The data in the converted RDD could be of org.apache.spark.sql.Row. 
     val ordersRDD = ordersDF.rdd
      
      
   
